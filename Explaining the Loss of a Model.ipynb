{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "colab": {
      "name": "Explaining the Loss of a Model.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/donaldtse118/shap_loss/blob/donald_dev/Explaining%20the%20Loss%20of%20a%20Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHaZbDdqs6X7"
      },
      "source": [
        "# Explaining the Loss of a Tree Model\n",
        "\n",
        "Explaining the loss of a model can be very useful for debugging and model monitoring. This notebook gives a very simple example of how this works. Note that explaining the loss of a model requires passing the labels, and is only supported for the `feature_dependence=\"independent\"` option of TreeExplainer.\n",
        "\n",
        "This notebook will be fleshed out once we post a full write-up of this method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RqlCj_dos_D6"
      },
      "source": [
        "#!pip install shap"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBf-ncyUs6YA"
      },
      "source": [
        "import shap\n",
        "import sklearn\n",
        "import xgboost\n",
        "import numpy as np"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTmMgWIDs6YA"
      },
      "source": [
        "### Train an XGBoost Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMYS0IVxs6YB",
        "outputId": "059b2ac7-467d-4910-8318-9684c9a019a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "X,y = shap.datasets.adult()\n",
        "\n",
        "model = xgboost.XGBClassifier()\n",
        "model.fit(X,y)\n",
        "\n",
        "# compute the logistic log-loss\n",
        "model_loss = -np.log(model.predict_proba(X)[:,1]) * y + -np.log(model.predict_proba(X)[:,0]) * (1-y)\n",
        "\n",
        "model_loss[:10]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.08443378, 0.45300266, 0.03874125, 0.11340553, 0.6735087 ,\n",
              "       1.41265261, 0.00916297, 0.91732287, 0.01906859, 0.07444511])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzYC44nPs6YB"
      },
      "source": [
        "### Explain the Log-Loss of the Model with TreeExplainer\n",
        "\n",
        "Note that the `expected_value` of the model's loss depends on the label and so it is now a function instead of a single number."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCkSVmlUs6YC",
        "outputId": "252f248a-c24c-40ac-e407-0b7ecb95b8cc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "explainer = shap.TreeExplainer(model, X, feature_dependence=\"independent\", model_output=\"log_loss\")\n",
        "explainer.shap_values(X.iloc[:10,:], y[:10]).sum(1) + np.array([explainer.expected_value(v) for v in y[:10]])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "feature_dependence = \"independent\" has been renamed to feature_perturbation = \"interventional\"! See GitHub issue #882.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.08443378, 0.45300268, 0.03874123, 0.11340551, 0.67350869,\n",
              "       1.41265219, 0.00916298, 0.9173229 , 0.01906863, 0.07444508])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HV3nepXVtlhX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}